import os
import glob
import json
import struct
import re
import numpy as np
import logging  # å¼•å…¥ logging æ¨¡å—
from tokenizers import Tokenizer

# ================= é…ç½®åŒºåŸŸ =================
TOKENIZER_JSON_PATH = "output_tokenizer/threebody_tokenizer.json"
DATASET_DIR = "datasets" 
OUTPUT_DIR = "output_tokenizer"
LOG_FILE = os.path.join(OUTPUT_DIR, "data_processing.log") # Log æ–‡ä»¶è·¯å¾„

# âš ï¸ å¿…é¡»æ£€æŸ¥ä½ çš„ json æ–‡ä»¶ï¼Œç¡®è®¤ <|endoftext|> çš„ ID æ˜¯å¤šå°‘
EOT_TOKEN_ID = 1  
# ===========================================

def clean_text(text):
    """
    æ–‡æœ¬æ¸…æ´—å‡½æ•° (ä¿æŒä¸å˜)
    """
    text = text.replace('\u3000', ' ')
    text = "".join(ch for ch in text if ch.isprintable() or ch in ['\n', '\t'])
    text = re.sub(r'\n\s*\n', '\n', text)
    text = text.strip()
    return text

def generate_dataset_bin():
    """
    ç”Ÿæˆæ•°æ®é›† bin æ–‡ä»¶ï¼Œå¹¶è®°å½•æ—¥å¿—
    """
    logging.info(">>> é˜¶æ®µ 1/2: å¼€å§‹ç”Ÿæˆæ•°æ®é›† bin æ–‡ä»¶")
    print(f"\n[1/2] æ­£åœ¨ç”Ÿæˆæ•°æ®é›† bin æ–‡ä»¶ (32-bit mode)...")
    
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    # 1. åŠ è½½åˆ†è¯å™¨
    if not os.path.exists(TOKENIZER_JSON_PATH):
        error_msg = f"æ‰¾ä¸åˆ° Tokenizer æ–‡ä»¶: {TOKENIZER_JSON_PATH}"
        logging.error(error_msg)
        raise FileNotFoundError(error_msg)
    
    print(f"Loading tokenizer: {TOKENIZER_JSON_PATH}")
    tokenizer = Tokenizer.from_file(TOKENIZER_JSON_PATH)
    vocab_size = tokenizer.get_vocab_size()
    
    msg = f"Tokenizer åŠ è½½æˆåŠŸ, Vocab size: {vocab_size}"
    print(msg)
    logging.info(msg)

    # 2. æ‰«ææ–‡ä»¶
    txt_files = glob.glob(os.path.join(DATASET_DIR, "*.txt"))
    if not txt_files:
        error_msg = f"åœ¨ {DATASET_DIR} ä¸‹æ²¡æœ‰æ‰¾åˆ° .txt æ–‡ä»¶ï¼"
        logging.error(error_msg)
        raise ValueError(error_msg)
    txt_files.sort()

    master_train = []
    master_val = []
    master_test = []

    print(f"å‘ç° {len(txt_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹é€ä¸ªæ¸…æ´—å¹¶åˆ’åˆ† (8:1:1)...")
    logging.info(f"å‘ç° {len(txt_files)} ä¸ªæºæ–‡ä»¶: {[os.path.basename(f) for f in txt_files]}")

    for file_path in txt_files:
        file_name = os.path.basename(file_path)
        print(f"  -> å¤„ç†: {file_name}")
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                raw_text = f.read()
            
            # --- æ­¥éª¤ A: æ¸…æ´— ---
            cleaned_text = clean_text(raw_text)
            
            # --- æ­¥éª¤ B: ç¼–ç  ---
            encoded = tokenizer.encode(cleaned_text)
            ids = encoded.ids
            
            # åœ¨æ¯æœ¬ä¹¦æœ«å°¾åŠ  EOT
            ids.append(EOT_TOKEN_ID)
            
            # --- æ­¥éª¤ C: æŒ‰æ–‡ä»¶åˆ’åˆ† (8:1:1) ---
            n_tokens = len(ids)
            if n_tokens < 10: 
                msg = f"è­¦å‘Š: {file_name} å†…å®¹å¤ªå°‘ ({n_tokens} tokens)ï¼Œè·³è¿‡åˆ’åˆ†ï¼Œå…¨éƒ¨æ”¾å…¥ Train"
                print(f"     âš ï¸ {msg}")
                logging.warning(msg)
                master_train.extend(ids)
                continue

            split_80 = int(n_tokens * 0.8)
            split_90 = int(n_tokens * 0.9)
            
            chunk_train = ids[:split_80]
            chunk_val   = ids[split_80:split_90]
            chunk_test  = ids[split_90:]
            
            master_train.extend(chunk_train)
            master_val.extend(chunk_val)
            master_test.extend(chunk_test)
            
            log_msg = f"æ–‡ä»¶ {file_name} å¤„ç†å®Œæ¯•: Train={len(chunk_train)}, Val={len(chunk_val)}, Test={len(chunk_test)} (Tokens)"
            print(f"     âœ… Train: {len(chunk_train)}, Val: {len(chunk_val)}, Test: {len(chunk_test)}")
            logging.info(log_msg)

        except Exception as e:
            msg = f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}"
            print(f"     âŒ {msg}")
            logging.error(msg)

    # 3. è½¬æ¢ä¸º numpy uint32
    print("\næ­£åœ¨è½¬æ¢ä¸º np.uint32...")
    train_data = np.array(master_train, dtype=np.uint32)
    val_data   = np.array(master_val,   dtype=np.uint32)
    test_data  = np.array(master_test,  dtype=np.uint32)

    # 4. å†™å…¥æ–‡ä»¶
    print("æ­£åœ¨å†™å…¥ç¡¬ç›˜...")
    train_path = os.path.join(OUTPUT_DIR, "train.bin")
    val_path   = os.path.join(OUTPUT_DIR, "val.bin")
    test_path  = os.path.join(OUTPUT_DIR, "test.bin")
    
    train_data.tofile(train_path)
    val_data.tofile(val_path)
    test_data.tofile(test_path)

    # 5. ç”Ÿæˆç»Ÿè®¡æ—¥å¿—
    print("-" * 30)
    print(f"ğŸ‰ æ•°æ®é›†ç”Ÿæˆå®Œæ¯•ï¼")
    print(f"  Train: {len(train_data)} tokens -> {train_path}")
    print(f"  Val  : {len(val_data)}   tokens -> {val_path}")
    print(f"  Test : {len(test_data)}  tokens -> {test_path}")

    padded_vocab = ((vocab_size + 63) // 64) * 64
    
    # å†™å…¥ info txt (ä¾› C è¯»å–)
    info_path = os.path.join(OUTPUT_DIR, "dataset_info.txt")
    with open(info_path, 'w', encoding='utf-8') as f:
        f.write(f"Vocab Size: {vocab_size}\n")
        f.write(f"Padded Vocab: {padded_vocab}\n")
        f.write(f"Train Tokens: {len(train_data)}\n")
        f.write(f"Val Tokens: {len(val_data)}\n")
        f.write(f"Test Tokens: {len(test_data)}\n")
    
    # å†™å…¥ Log
    logging.info("-" * 20)
    logging.info("æ•°æ®é›†æ±‡æ€»ç»Ÿè®¡:")
    logging.info(f"Total Train Tokens: {len(train_data)}")
    logging.info(f"Total Val Tokens  : {len(val_data)}")
    logging.info(f"Total Test Tokens : {len(test_data)}")
    logging.info(f"Padded Vocab Size : {padded_vocab} (Cè¯­è¨€å‚æ•° -v)")
    logging.info(f"Output files: {train_path}, {val_path}, {test_path}")
    
    print(f"ğŸ’¡ C è¯­è¨€è¿è¡Œå‚æ•°: -v {padded_vocab}")

def convert_tokenizer_bin():
    """
    å°† Tokenizer è¯è¡¨è½¬æ¢ä¸º C è¯­è¨€å¯è¯»çš„äºŒè¿›åˆ¶æ ¼å¼ (ä¿æŒä¸å˜ï¼Œå¢åŠ æ—¥å¿—)
    """
    logging.info(">>> é˜¶æ®µ 2/2: å¼€å§‹è½¬æ¢ Tokenizer bin")
    print(f"\n[2/2] æ­£åœ¨è½¬æ¢ Tokenizer è¯è¡¨ä¸ºäºŒè¿›åˆ¶...")
    json_path = TOKENIZER_JSON_PATH
    bin_path = os.path.join(OUTPUT_DIR, "threebody_tokenizer.bin")

    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    vocab_map = {}
    if "model" in data and "vocab" in data["model"]:
        vocab_map = data["model"]["vocab"]
    elif isinstance(data, dict):
        if "vocab" in data: vocab_map = data["vocab"]
        else: vocab_map = data
            
    if not vocab_map: 
        logging.error("JSON ä¸­æœªæ‰¾åˆ° vocab æ•°æ®")
        raise ValueError("JSON ä¸­æœªæ‰¾åˆ° vocab æ•°æ®")

    # å¯¹é½
    max_id = max(vocab_map.values()) if vocab_map else 0
    padded_size = ((max_id + 1 + 63) // 64) * 64
    print(f"Vocab Padded to: {padded_size}")
    logging.info(f"Original Max ID: {max_id}, Padded Size: {padded_size}")

    # å¡«å……ç©ºæ´
    token_list = [None] * padded_size
    for token, idx in vocab_map.items():
        if 0 <= idx < padded_size:
            token_list[idx] = token
            
    for i in range(padded_size):
        if token_list[i] is None:
            token_list[i] = f"<pad_{i}>"

    # å†™å…¥äºŒè¿›åˆ¶
    with open(bin_path, 'wb') as f:
        f.write(struct.pack('<I', 20260123)) 
        f.write(struct.pack('<I', 1)) 
        f.write(struct.pack('<I', padded_size)) 
        f.write(b'LJJ_GPT')
        f.write(b'\0' * (1024 - 12 - 7)) 
        
        logging.info("tokenizer_magic_number: 20260123")

        for token in token_list:
            b = token.encode('utf-8')
            length = len(b)
            if length > 255: length = 255
            if length == 0: length = 1; b = b'\0'
            f.write(struct.pack('<B', length))
            f.write(b[:length])

    msg = f"âœ… Tokenizer bin ç”Ÿæˆå®Œæ¯•: {bin_path}"
    print(msg)
    logging.info(msg)

if __name__ == "__main__":
    # --- 0. é…ç½®æ—¥å¿— ---
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        
    logging.basicConfig(
        filename=LOG_FILE,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        filemode='w' # æ¯æ¬¡è¿è¡Œè¦†ç›–æ—§æ—¥å¿—ï¼Œæƒ³è¿½åŠ è¯·æ”¹ä¸º 'a'
    )
    
    # è®°å½•å½“å‰è¿è¡Œé…ç½®
    logging.info("========================================")
    logging.info("å¼€å§‹æ‰§è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬")
    logging.info(f"TOKENIZER_JSON: {TOKENIZER_JSON_PATH}")
    logging.info(f"DATASET_DIR   : {DATASET_DIR}")
    logging.info(f"OUTPUT_DIR    : {OUTPUT_DIR}")
    logging.info(f"EOT_TOKEN_ID  : {EOT_TOKEN_ID}")
    logging.info("========================================")

    # --- 1. æ‰§è¡Œä»»åŠ¡ ---
    try:
        generate_dataset_bin()
        convert_tokenizer_bin()
        logging.info("æ‰€æœ‰ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
    except Exception as e:
        logging.critical(f"è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")