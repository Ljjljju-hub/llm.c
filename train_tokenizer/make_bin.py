import os
import numpy as np
from tokenizers import Tokenizer

# å°†æ•°æ®é›†çš„token idè½¬æ¢ä¸ºbin
def generate_bin():
    # å‡†å¤‡ç›®å½•
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    # --- 2. åŠ è½½åˆ†è¯å™¨ ---
    print(f"Loading tokenizer: {TOKENIZER_PATH}")
    tokenizer = Tokenizer.from_file(TOKENIZER_PATH)
    vocab_size = tokenizer.get_vocab_size()
    print(f"Vocab size: {vocab_size}")

    # --- 3. è¯»å–æ–‡æœ¬ ---
    print(f"Reading text: {INPUT_FILE}")
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        text = f.read()

    # --- 4. ç¼–ç  (æœ€å…³é”®ä¸€æ­¥) ---
    # æŠŠ "ç½—è¾‘" å˜æˆ [5000] è¿™æ ·çš„æ•°å­—
    print("Encoding text to IDs...")
    encoded = tokenizer.encode(text)
    ids = encoded.ids
    
    # æ·»åŠ ç»“æŸç¬¦ (End of Text)ï¼Œæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ ä½•æ—¶åœæ­¢
    # è¿™é‡Œçš„ 0 å¿…é¡»å¯¹åº”ä½  json æ–‡ä»¶é‡Œ <|endoftext|> çš„ ID
    # è¿™æ ·åˆ‡åˆ†åè®­ç»ƒé›†åæ˜¯æ²¡æœ‰id0çš„ã€‚
    # å½±å“ï¼šå¯¹äºã€Šä¸‰ä½“ã€‹å•è¡Œæœ¬è®­ç»ƒï¼Œå½±å“ä¸ºé›¶ã€‚æ¨¡å‹ä¼šæŠŠè®­ç»ƒé›†å½“æˆä¸€ä¸ªæ— é™å¾ªç¯çš„æ•…äº‹æ¥è¯»ã€‚
    ids.append(0) 
    
    total_tokens = len(ids)
    print(f"Total tokens: {total_tokens}")

    # --- 5. è½¬æ¢ä¸º uint16 (æ ¸å¿ƒä¼˜åŒ–) ---
    # å› ä¸ºä½ çš„è¯è¡¨çº¦ 20000 < 65535ï¼Œæ‰€ä»¥ç”¨ uint16 (2å­—èŠ‚) å­˜ã€‚
    # è¿™æ¯” Python é»˜è®¤çš„ int (é€šå¸¸8å­—èŠ‚) æˆ– int32 (4å­—èŠ‚) æå…¶èŠ‚çœç©ºé—´ã€‚
    data = np.array(ids, dtype=np.uint32)

    # --- 6. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† åˆ’åˆ†æ•°æ®é›† (80% / 10% / 10%) ---
    # æ³¨æ„è¿™é‡Œæ˜¯é¡ºåºåˆ’åˆ†
    # å¯¹äºå°è¯´ã€ä»£ç è¿™ç§å¼ºé€»è¾‘ã€å¼ºæ—¶åºçš„æ•°æ®ï¼Œå¿…é¡»åƒåˆ‡è›‹ç³•ä¸€æ ·ä¸€åˆ€åˆ‡ã€‚
    # åªæœ‰å¯¹äºäº’ä¸ç›¸å…³çš„ç‹¬ç«‹æ ·æœ¬ï¼ˆæ¯”å¦‚ 10000 æ¡ç‹¬ç«‹çš„å¾®åšè¯„è®ºåšæƒ…æ„Ÿåˆ†æï¼‰ï¼Œæ‰ä½¿ç”¨éšæœºåˆ’åˆ†ã€‚
    split_idx1 = int(total_tokens * 0.8)
    split_idx2 = int(total_tokens * 0.9)
    # è®­ç»ƒé›†: 0% -> 80%
    train_data = data[:split_idx1]
    # éªŒè¯é›†: 80% -> 90%
    val_data = data[split_idx1:split_idx2]
    # æµ‹è¯•é›†: 90% -> 100%
    test_data = data[split_idx2:]

    # --- 7. å†™å…¥ç¡¬ç›˜ ---
    # .tofile() ä¼šä¿å­˜çº¯äºŒè¿›åˆ¶æ•°æ®ï¼Œä¸å¸¦ä»»ä½•æ–‡ä»¶å¤´ä¿¡æ¯
    train_path = os.path.join(OUTPUT_DIR, "train.bin")
    val_path = os.path.join(OUTPUT_DIR, "val.bin")
    test_path = os.path.join(OUTPUT_DIR, "test.bin")
    
    train_data.tofile(train_path)
    val_data.tofile(val_path)
    test_data.tofile(test_path)

    print("-" * 30)
    print(f"ğŸ‰ ç”Ÿæˆå®Œæˆï¼")
    print(f"è®­ç»ƒé›†: {train_path} ({len(train_data)} tokens)")
    print(f"éªŒè¯é›†: {val_path} ({len(val_data)} tokens)")
    print(f"éªŒè¯é›†: {test_path} ({len(test_data)} tokens)")
    
    # è®¡ç®—ç»™ C è¯­è¨€ç”¨çš„è¯è¡¨å¤§å° (å¯¹é½åˆ° 64 çš„å€æ•°)
    padded_vocab = ((vocab_size + 63) // 64) * 64
    vocab_size_path = os.path.join(OUTPUT_DIR, "vocab_log.txt")
    with open(vocab_size_path, 'w', encoding='utf-8') as f:
        f.write(f"Total tokens={total_tokens}\n")
        f.write(f"è®­ç»ƒé›†: {train_path} ({len(train_data)} tokens)\n")
        f.write(f"éªŒè¯é›†: {val_path} ({len(val_data)} tokens)\n")
        f.write(f"éªŒè¯é›†: {test_path} ({len(test_data)} tokens)\n")
        f.write(f"vocab_size={vocab_size}\n")
        f.write(f"padded_vocab={padded_vocab}\n")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥è¿è¡Œ llm.c æ—¶ï¼Œè¯·ä½¿ç”¨å‚æ•°: -v {padded_vocab}")
    
import json
import struct
import sys
import os
# å°†è¯è¡¨è½¬æ¢ä¸ºbin

def convert_tokenizer(json_path, bin_path):
    print(f"æ­£åœ¨åŠ è½½ Tokenizer æ–‡ä»¶: {json_path}")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ---------------------------------------------------------
    # 1. æå–è¯è¡¨ (Vocab Extraction)
    # ---------------------------------------------------------
    vocab_map = {}
    
    # æƒ…å†µ A: HuggingFace æ ‡å‡† tokenizer.json ç»“æ„
    if "model" in data and "vocab" in data["model"]:
        # print("è¯†åˆ«ä¸º HuggingFace 'tokenizer.json' æ ¼å¼")
        vocab_map = data["model"]["vocab"]
    # æƒ…å†µ B: ç®€å•çš„é”®å€¼å¯¹ vocab.json
    elif isinstance(data, dict):
        first_val = next(iter(data.values()))
        if isinstance(first_val, int):
            # print("è¯†åˆ«ä¸ºç®€å•å­—å…¸æ ¼å¼ {'token': id}")
            vocab_map = data
        else:
            print("è­¦å‘Š: æ— æ³•è¯†åˆ«çš„ JSON ç»“æ„ï¼Œå°è¯•åœ¨æ ¹ç›®å½•å¯»æ‰¾ 'vocab' å­—æ®µ")
            if "vocab" in data:
                vocab_map = data["vocab"]
    
    if not vocab_map:
        raise ValueError("æ— æ³•åœ¨ JSON ä¸­æ‰¾åˆ°è¯è¡¨ (vocab) æ•°æ®ï¼")

    # ---------------------------------------------------------
    # 2. æ’åºä¸å¯¹é½ (Sorting & Alignment) --- ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ã€‘
    # ---------------------------------------------------------
    # æ‰¾å‡ºå®é™…æœ€å¤§çš„ ID
    max_id = max(vocab_map.values())
    original_vocab_size = max_id + 1
    
    # ã€å…³é”®æ­¥éª¤ã€‘å¼ºåˆ¶å‘ä¸Šå¯¹é½åˆ° 64 çš„å€æ•°
    # è¿™ä¼šæŠŠ 20000 å˜æˆ 20032ï¼Œä»è€ŒåŒ…å«æ¨¡å‹å¯èƒ½é¢„æµ‹å‡ºçš„ "è¶Šç•Œ" ID
    vocab_size = ((original_vocab_size + 63) // 64) * 64
    
    print(f"åŸå§‹è¯è¡¨å¤§å°: {original_vocab_size}")
    print(f"å¯¹é½åå¤§å° (Padding): {vocab_size} (è¿™å°†æ˜¯ C ç¨‹åºè¯»å–çš„å¤§å°)")
    
    # åˆå§‹åŒ–åˆ—è¡¨ï¼Œé•¿åº¦ä¸ºå¯¹é½åçš„å¤§å°
    token_list = [None] * vocab_size
    
    # å¡«å…¥çœŸå®çš„è¯
    for token_str, token_id in vocab_map.items():
        if 0 <= token_id < vocab_size:
            token_list[token_id] = token_str
        else:
            print(f"è­¦å‘Š: è·³è¿‡å¼‚å¸¸ ID {token_id}: {token_str}")

    # ---------------------------------------------------------
    # 3. å¡«è¡¥ç©ºæ´ (Gap Filling)
    # ---------------------------------------------------------
    fill_count = 0
    # éå†æ•´ä¸ªã€å¯¹é½åã€‘çš„å¤§å°
    for i in range(vocab_size):
        if token_list[i] is None:
            # ã€å…³é”®æ­¥éª¤ã€‘ç”¨ <pad_ID> å¡«è¡¥ç©ºä½
            # è¿™æ ·å½“ C ç¨‹åºè¯»åˆ° ID 20001 æ—¶ï¼Œä¼šæ‰“å° "<pad_20001>" è€Œä¸æ˜¯å´©æºƒ
            token_list[i] = f"<pad_{i}>"
            fill_count += 1
    
    if fill_count > 0:
        print(f"å·²è‡ªåŠ¨å¡«å…… {fill_count} ä¸ªç©ºæ´ (å« Padding)")

    # ---------------------------------------------------------
    # 4. å†™å…¥äºŒè¿›åˆ¶æ–‡ä»¶ (Binary Writing)
    # ---------------------------------------------------------
    print(f"æ­£åœ¨å†™å…¥äºŒè¿›åˆ¶æ–‡ä»¶: {bin_path}")
    
    with open(bin_path, 'wb') as f:
        # --- Header (1024 bytes) ---
        f.write(struct.pack('<I', 20260123))   # Magic
        f.write(struct.pack('<I', 1))          # Version
        f.write(struct.pack('<I', vocab_size)) # ã€æ³¨æ„ã€‘å†™å…¥çš„æ˜¯å¯¹é½åçš„å¤§å°(20032)
        f.write(b'LJJ\x00')                    # Creator
        
        # å¡«å…… Header å‰©ä½™éƒ¨åˆ†
        for _ in range(252):
            f.write(struct.pack('<I', 0))

        # --- Body (Token Data) ---
        for i, token_str in enumerate(token_list):
            # å¤„ç†ç‰¹æ®Šå­—ç¬¦
            token_str = token_str.replace('Ä ', ' ') 
            token_bytes = token_str.encode('utf-8')
            length = len(token_bytes)
            
            # C ä»£ç é™åˆ¶é•¿åº¦å¿…é¡»æ˜¯ 0-255
            if length > 255:
                token_bytes = token_bytes[:255]
                length = 255
            elif length == 0:
                token_bytes = b'\0'
                length = 1 

            f.write(struct.pack('<B', length))
            f.write(token_bytes)

    print("âœ… è¯è¡¨è½¬æ¢å®Œæˆï¼")

if __name__ == "__main__":
    # --- 1. é…ç½®è·¯å¾„ (è¯·æ ¹æ®ä½ çš„å®é™…æƒ…å†µä¿®æ”¹) ---
    TOKENIZER_PATH = "output_tokenizer/threebody_tokenizer.json" # ä½ åˆšæ‰ç”Ÿæˆçš„json
    INPUT_FILE = "datasets/ä¸‰ä½“å…¨é›†"                         # ä½ çš„å°è¯´txt
    OUTPUT_DIR = "output_tokenizer"                         # å‡†å¤‡å­˜æ”¾binæ–‡ä»¶çš„ç›®å½•
    # å°†æ•°æ®é›†token id è½¬æ¢ä¸ºbin
    generate_bin()
    
    # å°†è¯è¡¨è½¬æ¢ä¸ºbin
    output_bin = os.path.join(OUTPUT_DIR, "threebody_tokenizer.bin") # è¾“å‡ºçš„ BIN æ–‡ä»¶è·¯å¾„

    if not os.path.exists(TOKENIZER_PATH):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {TOKENIZER_PATH}")
    else:
        try:
            convert_tokenizer(TOKENIZER_PATH, output_bin)
        except Exception as e:
            print(f"è½¬æ¢å¤±è´¥: {e}")