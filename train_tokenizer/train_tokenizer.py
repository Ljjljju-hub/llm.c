from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors
import os
import re
from datetime import datetime

# å®šä¹‰åŸºç¡€æ–‡ä»¶å
BASE_MODEL_NAME = "threebody_tokenizer"
# ä¿å­˜æ–‡ä»¶å¤¹
output_dir = "output_tokenizer"

def train_threebody_tokenizer(data_file):
    # 1. åˆå§‹åŒ– Tokenizer
    # ä½¿ç”¨ BPE æ¨¡å‹ï¼ˆGPT-2/3/4, LLaMA åŒæ¬¾æ ¸å¿ƒç®—æ³•ï¼‰
    tokenizer = Tokenizer(models.BPE())

    # 2. é¢„å¤„ç† (Pre-tokenization)
    # ByteLevel æå…¶é‡è¦ï¼šå®ƒå°†å­—ç¬¦è½¬åŒ–ä¸ºå­—èŠ‚ã€‚
    # è¿™æ„å‘³ç€ä»»ä½• Unicode å­—ç¬¦ï¼ˆåŒ…æ‹¬ç”Ÿåƒ»æ±‰å­—ï¼‰éƒ½èƒ½è¢«å¤„ç†ï¼Œä¸ä¼šå‡ºç° [UNK]
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)

    # 3. è§£ç å™¨ (Decoder)
    # ç”¨äºå°† ID è½¬å›æ–‡æœ¬æ—¶ï¼ŒæŠŠå­—èŠ‚è¿˜åŸæˆå­—ç¬¦
    tokenizer.decoder = decoders.ByteLevel()

    # 4. è®¾ç½®è®­ç»ƒå™¨ (Trainer)
    # vocab_size: è¯è¡¨å¤§å°ã€‚
    # ã€Šä¸‰ä½“ã€‹å…¨æ–‡çº¦ 80-90ä¸‡å­—ã€‚
    # å¸¸è§çš„ä¸­æ–‡ LLM è¯è¡¨åœ¨ 3ä¸‡-10ä¸‡ä¹‹é—´ã€‚
    # å¯¹äºçº¯ã€Šä¸‰ä½“ã€‹è¯­æ–™ï¼Œè®¾ä¸º 10,000 - 20,000 è¶³å¤Ÿæ•æ‰å¸¸ç”¨è¯å’Œäººåï¼ˆå¦‚â€œç½—è¾‘â€ã€â€œäº‘å¤©æ˜â€ï¼‰ã€‚
    trainer = trainers.BpeTrainer(
        vocab_size=20000, 
        min_frequency=2,  # è‡³å°‘å‡ºç°2æ¬¡æ‰ä¼šè¢«æ”¶å½•
        special_tokens=[
            # --- åŸºç¡€æ§åˆ¶ç¬¦ ---
            "<|endoftext|>",  # æ–‡æ¡£ç»“æŸ/EOS (End of Sentence)
            "<|padding|>",    # å¡«å……ç¬¦/PAD (Padding)
            
            # --- å¯¹è¯/æŒ‡ä»¤å¾®è°ƒä¸“ç”¨ç¬¦ (ChatMLé£æ ¼) ---
            "<|im_start|>",   # æ ‡è®°ä¸€å¥è¯çš„å¼€å§‹
            "<|im_end|>",     # æ ‡è®°ä¸€å¥è¯çš„ç»“æŸ (éå¸¸é‡è¦ï¼Œé˜²æ­¢æ¨¡å‹è‡ªè¨€è‡ªè¯­åœä¸ä¸‹æ¥)
            
            # --- è§’è‰²æ ‡è¯†ç¬¦ (æ˜¾å¼å ä½) ---
            "<|system|>",     # ç³»ç»Ÿæç¤ºè¯ (System Prompt)
            "<|user|>",       # ç”¨æˆ·è¾“å…¥
            "<|assistant|>",  # AIè¾“å‡º
            
            # --- æ€è€ƒ/æ€ç»´é“¾ä¸“ç”¨ (Optional, ç±»ä¼¼ DeepSeek-R1) ---
            "<|thought|>",    # å¼€å§‹æ€è€ƒ
            "<|/thought|>"    # ç»“æŸæ€è€ƒ
        ], # ç‰¹æ®Šç¬¦å·
        show_progress=True
    )

    # 5. å¼€å§‹è®­ç»ƒ
    print(f"å¼€å§‹è®­ç»ƒ Tokenizerï¼Œè¯»å–æ–‡ä»¶: {data_file} ...")
    tokenizer.train([data_file], trainer=trainer)

    # 6. åå¤„ç† (Post-processing) - å¯é€‰
    # åœ¨ BPE ä¹‹å‰é€šå¸¸ä¸éœ€è¦å¤æ‚çš„ post-processingï¼Œä½†åœ¨ä¿å­˜å‰æœ€å¥½ç¡®è®¤ä¸€ä¸‹
    
    # 7. ä¿å­˜
    # 2. ã€å…³é”®ã€‘å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œå¿…é¡»å…ˆåˆ›å»ºå®ƒï¼
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    save_path = os.path.join(output_dir,"threebody_tokenizer.json")
    tokenizer.save(save_path)
    print(f"è®­ç»ƒå®Œæˆï¼Tokenizer å·²ä¿å­˜è‡³: {save_path}")
    return tokenizer

# --- æ‰§è¡Œè®­ç»ƒ ---
# è¯·ç¡®ä¿å½“å‰ç›®å½•ä¸‹æœ‰ three_body.txt æ–‡ä»¶
if __name__ == "__main__":
    # å¦‚æœä½ æœ‰ä¸‰ä¸ªæ–‡ä»¶ï¼Œå¯ä»¥ä¼ å…¥åˆ—è¡¨ï¼š["part1.txt", "part2.txt", "part3.txt"]
    model_path = os.path.join(output_dir, "threebody_tokenizer.json")
    if not os.path.exists(model_path):
        tokenizer = train_threebody_tokenizer("./datasets/ä¸‰ä½“å…¨é›†")
    else:
        print(f"ğŸ” å‘ç°å·²è®­ç»ƒå¥½çš„æ¨¡å‹: {model_path}")
        print("ğŸ“‚ æ­£åœ¨ç›´æ¥åŠ è½½...")
        
        # --- ã€æ ¸å¿ƒä»£ç ã€‘åŠ è½½æ¨¡å‹ ---
        tokenizer = Tokenizer.from_file(model_path)
        print("âœ… åŠ è½½æˆåŠŸï¼")
    # --- æµ‹è¯•ä¸€ä¸‹æ•ˆæœ ---
    test_text = "ä¸è¦å›ç­”ï¼ä¸è¦å›ç­”ï¼ä¸è¦å›ç­”ï¼è¿™æ˜¯å¶æ–‡æ´å‘å‡ºçš„è­¦å‘Šã€‚"
    encoded = tokenizer.encode(test_text)
    
    print("-" * 30)
    print(f"æµ‹è¯•æ–‡æœ¬: {test_text}")
    print(f"åˆ†è¯ç»“æœ (Tokens): {encoded.tokens}")
    decoded_text = tokenizer.decode(encoded.ids)
    for i in encoded.ids:
        print(f"è§£ç ç»“æœ: {tokenizer.decode([i])}\n")
        
    print(f"å¯¹åº”çš„ IDs: {encoded.ids}")
    
    # éªŒè¯æ˜¯å¦æ”¶å½•äº†ä¸“æœ‰åè¯
    name_test = "ç½—è¾‘ç›´æ¥å‘ä¸‰ä½“ä¸–ç•Œå‘å‡ºäº†å¨æ…‘ã€‚"
    encoded_name = tokenizer.encode(name_test)
    print(f"\nä¸“æœ‰åè¯æµ‹è¯•: {name_test}")
    print(f"åˆ†è¯ç»“æœ: {encoded_name.tokens}")
    decoded_text = tokenizer.decode(encoded.ids)
    print(f"è§£ç ç»“æœ: {decoded_text}")
    # è§‚å¯Ÿ 'ç½—è¾‘' æ˜¯å¦è¢«åˆå¹¶ä¸ºä¸€ä¸ª Tokenï¼Œè¿˜æ˜¯åˆ†æˆäº† 'ç½—' å’Œ 'è¾‘'