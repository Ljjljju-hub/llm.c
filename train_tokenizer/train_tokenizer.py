from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors
import os
import glob
import re

# --- é…ç½® ---
BASE_MODEL_NAME = "threebody_tokenizer"
OUTPUT_DIR = "output_tokenizer"
DATASET_DIR = "./datasets"
TEMP_CORPUS_FILE = "temp_merged_corpus.txt" # ä¸´æ—¶åˆå¹¶æ–‡ä»¶

def clean_text(text):
    """
    æ•°æ®æ¸…æ´—å‡½æ•°ï¼šåªä¿ç•™æœ‰ç”¨çš„æ–‡æœ¬
    """
    # 1. æ›¿æ¢å…¨è§’ç©ºæ ¼ä¸ºåŠè§’ç©ºæ ¼ (å¾ˆå¤šä¸­æ–‡å°è¯´ä¼šæœ‰ \u3000)
    text = text.replace('\u3000', ' ')
    
    # 2. å»é™¤ä¸å¯è§å­—ç¬¦ (é™¤äº†æ¢è¡Œç¬¦ \n å’Œ åˆ¶è¡¨ç¬¦ \t)
    # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†é˜²æ­¢ weird control characters è¿›å…¥è¯è¡¨
    text = "".join(ch for ch in text if ch.isprintable() or ch in ['\n', '\t'])

    # 3. æŠŠè¿ç»­çš„å¤šä¸ªç©ºæ ¼å˜æˆä¸€ä¸ªç©ºæ ¼ (å¯é€‰ï¼Œçœ‹ä½ æ˜¯å¦åœ¨æ„ç¼©è¿›)
    text = re.sub(r'\s+', ' ', text) 
    
    # 4. å»é™¤è¿ç»­çš„ç©ºè¡Œ (ä¿ç•™æ®µè½ç»“æ„ï¼Œä½†å»é™¤å¤§æ®µç©ºç™½)
    text = re.sub(r'\n\s*\n', '\n', text)
    
    return text

def merge_and_clean_files(source_dir, output_file):
    """
    è¯»å–ç›®å½•ä¸‹æ‰€æœ‰txtï¼Œæ¸…æ´—ååˆå¹¶åˆ°ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶
    """
    # æ‰¾åˆ°æ‰€æœ‰ txt æ–‡ä»¶
    files = glob.glob(os.path.join(source_dir, "*.txt"))
    if not files:
        raise ValueError(f"åœ¨ {source_dir} ä¸‹æ²¡æœ‰æ‰¾åˆ° .txt æ–‡ä»¶ï¼")
    
    print(f"ğŸ“š å‘ç° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå‡†å¤‡åˆå¹¶æ¸…æ´—...")
    
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for file_path in files:
            print(f"  -> å¤„ç†: {os.path.basename(file_path)}")
            try:
                # errors='ignore' é˜²æ­¢å› ä¸ºæŸä¸ªå­—ç¼–ç é”™è¯¯å¯¼è‡´æ•´ä¸ªè„šæœ¬å´©æºƒ
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:
                    content = infile.read()
                    cleaned_content = clean_text(content)
                    outfile.write(cleaned_content)
                    # æ¯ä¸ªæ–‡ä»¶ä¹‹é—´åŠ ä¸ªæ¢è¡Œï¼Œé˜²æ­¢å‰ä¸€æœ¬ä¹¦çš„ç»“å°¾å’Œåä¸€æœ¬ä¹¦çš„å¼€å¤´è¿åœ¨ä¸€èµ·
                    outfile.write("\n") 
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {file_path}: {e}")
    
    print(f"âœ… åˆå¹¶å®Œæˆï¼Œç”Ÿæˆä¸´æ—¶è¯­æ–™: {output_file}")
    return output_file

def train_tokenizer(corpus_file):
    # 1. åˆå§‹åŒ– Tokenizer (BPE)
    tokenizer = Tokenizer(models.BPE(unk_token="[UNK]"))

    # 2. é¢„å¤„ç† (ByteLevel)
    tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()

    # 3. è§£ç å™¨
    tokenizer.decoder = decoders.BPEDecoder()

    # 4. è®­ç»ƒå™¨é…ç½®
    # æ³¨æ„ï¼šå¦‚æœè¯­æ–™å˜å¤§äº†ï¼ˆåˆ˜æ…ˆæ¬£å…¨é›†ï¼‰ï¼Œ20000 ä¾ç„¶æ˜¯åˆç†çš„ï¼Œ
    # ä½†å¦‚æœè¯­æ–™æå…¶å·¨å¤§ï¼ˆGBçº§åˆ«ï¼‰ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘ 30000-50000ã€‚
    trainer = trainers.BpeTrainer(
        vocab_size=20000, 
        min_frequency=2,
        special_tokens=[
            "[UNK]",
            "<|endoftext|>", "<|padding|>", 
            "<|im_start|>", "<|im_end|>", 
            "<|system|>", "<|user|>", "<|assistant|>",
            "<|thought|>", "<|/thought|>"
        ],
        show_progress=True
    )

    # 5. å¼€å§‹è®­ç»ƒ
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ Tokenizerï¼Œè¯»å–åˆå¹¶è¯­æ–™: {corpus_file} ...")
    # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä¼ å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    tokenizer.train([corpus_file], trainer=trainer)

    # 6. ä¿å­˜
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    save_path = os.path.join(OUTPUT_DIR, "threebody_tokenizer.json")
    tokenizer.save(save_path)
    print(f"ğŸ’¾ è®­ç»ƒå®Œæˆï¼å·²ä¿å­˜è‡³: {save_path}")
    return tokenizer

if __name__ == "__main__":
    model_path = os.path.join(OUTPUT_DIR, "threebody_tokenizer.json")
    
    # å¼ºåˆ¶é‡æ–°è®­ç»ƒçš„å¼€å…³ï¼ˆå¦‚æœä½ æƒ³æ›´æ–°è¯è¡¨ï¼Œè®¾ä¸º Trueï¼‰
    FORCE_RETRAIN = False 

    if not os.path.exists(model_path) or FORCE_RETRAIN:
        # 1. æ¸…æ´—å¹¶åˆå¹¶æ•°æ®
        merge_and_clean_files(DATASET_DIR, TEMP_CORPUS_FILE)
        
        # 2. è®­ç»ƒ
        tokenizer = train_tokenizer(TEMP_CORPUS_FILE)
        
        # 3. åˆ é™¤ä¸´æ—¶æ–‡ä»¶ (å¯é€‰)
        if os.path.exists(TEMP_CORPUS_FILE):
            os.remove(TEMP_CORPUS_FILE)
            print("ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶åˆå¹¶æ–‡ä»¶")
    else:
        print(f"ğŸ” å‘ç°å·²å­˜åœ¨çš„æ¨¡å‹: {model_path}")
        tokenizer = Tokenizer.from_file(model_path)
        print("âœ… åŠ è½½æˆåŠŸï¼")

    # --- æµ‹è¯•ç¯èŠ‚ ---
    print("\n" + "="*30)
    # æµ‹è¯•ä¸€äº›å°è¯´é‡Œå¸¸è§çš„è¯ï¼Œçœ‹çœ‹å®ƒä»¬æ˜¯ä¸€ä¸ª Token è¿˜æ˜¯è¢«æ‹†åˆ†äº†
    test_sentences = [
        "ä¸è¦å›ç­”ï¼ä¸è¦å›ç­”ï¼ä¸è¦å›ç­”ï¼",
        "ç½—è¾‘ç›´æ¥å‘ä¸‰ä½“ä¸–ç•Œå‘å‡ºäº†å¨æ…‘ã€‚",
        "ç»™å²æœˆä»¥æ–‡æ˜ï¼Œè€Œä¸æ˜¯ç»™æ–‡æ˜ä»¥å²æœˆã€‚", # é»‘æš—æ£®æ—åè¨€
        "å¼±å°å’Œæ— çŸ¥ä¸æ˜¯ç”Ÿå­˜çš„éšœç¢ï¼Œå‚²æ…¢æ‰æ˜¯ã€‚", # æ­»ç¥æ°¸ç”Ÿåè¨€
        "ç« åŒ—æµ·å¾®å¾®ä¸€ç¬‘ã€‚",
        "è¿™æ˜¯åˆ˜æ…ˆæ¬£çš„ç§‘å¹»å°è¯´å…¨é›†ã€‚"
    ]

    for text in test_sentences:
        encoded = tokenizer.encode(text)
        print(f"\nåŸæ–‡: {text}")
        print(f"Tokens: {encoded.tokens}")
        print(f"IDs:    {encoded.ids}")