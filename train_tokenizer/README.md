# 训练流程记录：Tokenizer 与数据预处理

本项目用于记录 `llm.c` (GPT-2) 架构下的自定义分词器训练及数据二进制化流程。

## 1. 特殊符号定义 (Special Tokens)

为了支持后续的指令微调 (Chat) 和思维链 (CoT) 能力，在词表中预留了以下特殊符号（IDs 0-8）。

| ID | 符号 | 说明 | 备注 |
| --- | --- | --- | --- |
| **0** | `<|endoftext|>` |
| **1** | `<|padding|>` |
| **2** | `<|im_start|>` |
| **3** | `<|im_end|>` |
| **4** | `<|system|>` |
| **5** | `<|user|>` |
| **6** | `<|assistant|>` |
| **7** | `<|thought|>` |
| **8** | `<|/thought|>` |

---

## 2. 阶段一：训练分词器

### 脚本

`train_tokenizer.py`

### 输入 (Input)

* **格式**：纯文本文件 (`.txt`)
* **编码**：UTF-8
* **内容**：原始训练语料（如小说全集、百科数据等）。
* **示例路径**：`./datasets/三体全集.txt`

### 处理逻辑

1. **模型**：BPE (Byte-Pair Encoding)。
2. **预处理**：ByteLevel (将字符转为 UTF-8 字节，防止 UNK)。
3. **词表大小**：约 20,000 (根据语料规模设定，适合中等中文语料)。
4. **过滤**：`min_frequency=2` (过滤掉仅出现一次的噪点)。

### 输出 (Output)

* **格式**：JSON 文件
* **内容**：包含 `vocab` (ID映射表) 和 `merges` (合并规则)。
* **示例路径**：`./output_tokenizer/threebody_tokenizer.json`

---

## 3. 阶段二：生成二进制数据 (.bin)

### 脚本

`make_bin.py`

### 输入 (Input)

1. **分词器**：上一步生成的 `threebody_tokenizer.json`。
2. **原始语料**：同上，`./datasets/三体全集.txt`。

### 处理逻辑

1. **加载**：读取 Tokenizer 配置。
2. **编码**：将文本转换为 Token ID 列表 (`List[int]`)。
3. **追加**：在末尾添加 `<|endoftext|>` (ID: 0)。
4. **类型转换**：转换为 `numpy.uint16` 格式。
* *原因*：词表 < 65535，使用 16 位整数存储更节省显存。


5. **划分**：按 8:1:1 比例划分为训练集和验证集。

### 输出 (Output)

* **格式**：Raw Binary (无文件头，纯二进制流)
* **文件**：
* `train.bin` (训练数据)
* `val.bin` (验证数据)
* `test_data`(测试数据)


* **路径**：`./output_tokenizer/`

---

## 4. 接入 llm.c 训练说明

在运行 C 语言训练程序时，需注意以下参数配置：

### 词表对齐 (重要)

虽然 Tokenizer 的实际词表大小约为 20,009，但为了 CUDA 矩阵运算的性能（对齐），建议将 `-v` 参数设置为 **64 的倍数**。

* **计算公式**：`((Vocab_Size + 63) // 64) * 64`
* **推荐设置**：`-v 20032` (针对 20k 词表)

# threebody_tokenizer.bin结构说明

---

# GPT-2 Tokenizer 二进制格式说明 (`gpt2_tokenizer.bin`)

`gpt2_tokenizer.bin` 是为 `llm.c` 项目专门设计的自定义二进制文件格式。该格式旨在通过极简的结构设计，实现 C 语言环境下**零依赖**、**高性能**的词表读取。

文件结构严格固定（Strict Layout），由 **Header（文件头）** 和 **Body（数据体）** 两部分组成。

## 1. 文件结构概览

```text
[      Header (固定 1024 字节)      ]
+-----------------------------------+
| Magic Number (4 bytes) = 20240328 | -> 文件魔数，用于校验
| Version      (4 bytes) = 1        | -> 格式版本号
| Vocab Size   (4 bytes) = N        | -> 词表大小 (例如 50257)
| ... Padding (填充数据) ...         | -> 填充 0 至 1024 字节对齐
+-----------------------------------+

[        Body (紧接 Header)         ]
+-----------------------------------+
| Token 0 Length (1 byte)           | -> 第 0 个词的字节长度
| Token 0 String (Len bytes)        | -> 第 0 个词的 UTF-8 内容
+-----------------------------------+
| Token 1 Length (1 byte)           |
| Token 1 String (Len bytes)        |
+-----------------------------------+
| ... 循环读取 N 个 Token ...        |
+-----------------------------------+

```

---

## 2. 字段详细说明

### 2.1 Header 部分 (固定 1024 字节)

Header 本质上是一个长度为 256 的 `int32` (32位整数) 数组，总占用  字节。

| 偏移量 (Offset) | 字段名 | 类型 | 值/说明 |
| --- | --- | --- | --- |
| **0x00** | **Magic Number** | `uint32` | `0x0134C5D8` (十进制 `20240328`)。<br>

<br>用于安全校验。若读取值不匹配，程序将报错退出，防止错误加载其他文件（如权重文件或图片）。 |
| **0x04** | **Version** | `uint32` | `1`。<br>

<br>当前格式版本号。用于未来格式变更时的兼容性判断。 |
| **0x08** | **Vocab Size** | `uint32` | `N` (例如 50257 或 20000)。<br>

<br>定义了 Body 部分包含的 Token 总数。 |
| **0x0C - 0x3FF** | **Padding** | `uint32[]` | **保留位**。<br>

<br>目前全部填充为 `0`。保留此空间是为了保证 Header 严格对齐到 1024 字节，方便未来扩展元数据而不破坏整体结构。 |

### 2.2 Body 部分 (变长数据)

Body 部分由紧凑排列的 **“长度-内容”** 对组成。数据流中无分隔符，完全依赖“长度”字段进行切分。

对于每一个 Token  ()，存储结构如下：

1. **Length (1 byte)**: `unsigned char`
* 表示紧随其后的 Token 字符串的字节数。
* **注意**：由于使用单字节存储长度，这意味着该格式限制**单个 Token 的 UTF-8 编码长度不能超过 255 字节**。


2. **Content (Length bytes)**: `char[]`
* Token 的原始 UTF-8 字节流。
* 例如：中文字符 "三" 在此处存储为 3 个字节 `\xE4\xB8\x89`。
* *实现细节*：C 程序在读取这 `Length` 个字节后，会自动在内存中追加一个 `\0` (Null Terminator)，将其转换为合法的 C 字符串。



---

## 3. 设计理念 (Design Rationale)

采用这种设计主要基于以下考量：

1. **零依赖 (Zero Dependency)**:
无需引入 `json-c`、`protobuf` 或其他复杂的序列化库，仅需 C 标准库中的 `fread` 即可完成解析。
2. **极速解析 (Fast)**:
读取过程是线性的内存拷贝，避免了复杂的字符串解析（Parsing）和转义处理开销。
3. **内存友好 (Memory Friendly)**:
程序可以一边读取一边进行 `malloc`，能够精确控制内存分配，避免读取大文本文件时的内存抖动。

---

