--- Config ---
Logging to: train_log_2026-01-28.txt
Model: output_pre_model/gpt2_init.bin -> output_model/final_model.bin
Data : output_tokenizer/train.bin (Train), output_tokenizer/val.bin (Val)
Dims : B=8, T=128
Steps: 50 (Val every 100)
----------------
[GPT-2]
max_seq_len: 512
vocab_size: 20032
num_layers: 6
num_heads: 8
channels: 512
num_parameters: 29433856
step 0 val loss 10.007337
step 0: train loss 9.989798 (took 2224.298331 ms)
step 1: train loss 9.423325 (took 1732.378383 ms)
step 2: train loss 9.205328 (took 1557.830018 ms)
step 3: train loss 9.091440 (took 1666.943167 ms)
step 4: train loss 9.039591 (took 1612.665046 ms)
step 5: train loss 8.754354 (took 1598.027488 ms)
step 6: train loss 8.707777 (took 1611.112949 ms)
step 7: train loss 8.755823 (took 1618.486403 ms)
step 8: train loss 8.857678 (took 1683.509604 ms)
step 9: train loss 8.581650 (took 1829.170112 ms)
step 10 val loss 8.727373
step 10: train loss 8.465640 (took 1763.972436 ms)
step 11: train loss 8.472786 (took 1705.353286 ms)
step 12: train loss 8.448380 (took 1748.373490 ms)
step 13: train loss 8.256783 (took 1767.821483 ms)
step 14: train loss 8.249886 (took 1665.461748 ms)
step 15: train loss 8.339602 (took 1702.336685 ms)
step 16: train loss 8.350122 (took 1700.769411 ms)
step 17: train loss 8.476027 (took 1711.564716 ms)
step 18: train loss 8.293506 (took 1682.663128 ms)
step 19: train loss 8.361968 (took 1758.392119 ms)
step 20 val loss 8.533351
generating at step 20:
---
æĪĲâĢľå±Ĥå°±çĶ¨æĺ¯ä¸Ģé¢Ĺä¸Ģæł·è¾¹æĺ¯ä¸ªç¼ĺç§ĳæĬĢä¼½æľ¬èº«çļĦéĺ´å½±è¿ĻäººâĢľå¥¹çļĦâĢľåĩłåı¥çļĦäººçļĦèĭ¦åľ¨ä¸ĢĊè¯´ä»ĸä»Ķç»Ĩï¼Įå¤ļçļĦè¿ĺè¦ģçļĦåľ°åĿĢåĲĳåħ³ä¸Ĭï¼Įæĭ·ï¼ĮâĢľ¸èħ¾è®¡ĊãĢĤæŁĶåĴĮçĤ½ä¸Ńæľīä¸Ģä¸ªå·ŀçļĦæ¡æļĹä¸ĭåİ»ãĢĤç¬ĶçĽ´00å¥¹çļĦçľ¼çĿĽ647ï¼Įå±±çļĦä¸ĵå®¶ï¼Įè¿ĲçĶ¨æľĢåĲİè¯´çļĦæĶ¿åºľéļ¾éģĵĊãĢĬèĢĮè¿ĻäºĽæĿĢäººåıĪéĹ®ä½łè¯´
---
step 20: train loss 8.188738 (took 1800.661101 ms)
step 21: train loss 8.375813 (took 1720.908723 ms)
step 22: train loss 8.654634 (took 1737.071223 ms)
step 23: train loss 8.503153 (took 1719.334482 ms)
step 24: train loss 8.316003 (took 1731.060201 ms)
step 25: train loss 8.606222 (took 1725.685501 ms)
step 26: train loss 8.277424 (took 1710.259516 ms)
step 27: train loss 8.328840 (took 1733.846476 ms)
step 28: train loss 8.111828 (took 1783.091900 ms)
step 29: train loss 8.158456 (took 1857.675656 ms)
step 30 val loss 8.528376
step 30: train loss 8.142632 (took 1876.502889 ms)
step 31: train loss 8.286826 (took 1993.484573 ms)
step 32: train loss 8.112375 (took 1744.341087 ms)
step 33: train loss 8.073956 (took 1786.433407 ms)
step 34: train loss 8.083599 (took 1701.078207 ms)
step 35: train loss 7.823606 (took 1708.973713 ms)
step 36: train loss 8.015200 (took 1672.404067 ms)
step 37: train loss 7.868068 (took 1741.896833 ms)
step 38: train loss 7.861023 (took 1693.797385 ms)
step 39: train loss 8.182052 (took 1696.448504 ms)
step 40 val loss 8.497832
generating at step 40:
---
æĺ¯è¿Ļæł·çļĦåĴĮæķıæĦŁ198ï¼Įï¼ĮåĬ³åĬ¨ï¼ĮæĬ½åĩºä¸Ģï¼Įæ¥¼çļĦæĪĳä»¬æĺ¯çļĦéĤ£æľ¨ãĢĤâĢĿç¿»ä»¶ä¸ľè¥¿åħ±ä¹Łèĥ½ãĢĤèĦ±æ°´è¯¦ç»ĨçĶļèĩ³åı¯èĥ½åĨįâĢĻæīĭä¸ŃâĢľçĦļâĢľãĢĤĊè¯´è¯ĿçŁ¥åľ¨å¤©åŃĲæĹģè¾¹åľ¨è¿ĻåĦ¿çĻ½çĲĥä»ĸä»¬çĶ¨å½¢æĢģåĲĮæĹ¶å°±è·ŁåĬłå¿«éļĲèĹıãĢĤä¸įï¼Įä½łäºĨè¿ĳçªģçĦ¶ï¼Įçģµï¼ĮåĪ°æĮĩä»¤ï¼Įä¸Ńæľīè§Ħå¾ĭåĨĴï¼Įè§ĦåĪĻâĢ¦âĢ¦
---
step 40: train loss 8.057266 (took 1798.967455 ms)
step 41: train loss 8.043455 (took 1759.302446 ms)
step 42: train loss 8.238358 (took 1736.347922 ms)
step 43: train loss 8.101158 (took 1718.201149 ms)
step 44: train loss 8.406612 (took 1748.028274 ms)
step 45: train loss 8.897188 (took 1830.657180 ms)
step 46: train loss 8.385759 (took 2223.018795 ms)
step 47: train loss 8.129295 (took 1919.182390 ms)
step 48: train loss 8.067574 (took 1770.265002 ms)
step 49: train loss 8.264880 (took 1754.058665 ms)
step 50 val loss 8.514097
step 50: train loss 7.928235 (took 1803.129007 ms)
